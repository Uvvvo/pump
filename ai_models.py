"""
Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨ÙØ´Ù„ Ø§Ù„Ù…Ø¶Ø®Ø§Øª
"""

import numpy as np
import pandas as pd
import joblib
import json
from sklearn.ensemble import RandomForestClassifier, IsolationForest, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.impute import SimpleImputer
import xgboost as xgb
from datetime import datetime, timedelta
import logging
from typing import Dict, List, Tuple, Any, Optional
import warnings
from pathlib import Path
import sqlite3
from contextlib import contextmanager

from config import AI_MODELS_CONFIG, PUMP_CONFIG

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª
warnings.filterwarnings('ignore')

class ModelManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªÙ†Ø¨Ø¤"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.model_history = []
    
    def _setup_logger(self) -> logging.Logger:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„"""
        logger = logging.getLogger(__name__)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def save_model_metadata(self, model, accuracy: float, features: List[str]):
        """Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØµÙÙŠØ© Ø¹Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        metadata = {
            'timestamp': datetime.now().isoformat(),
            'accuracy': accuracy,
            'features': features,
            'model_type': type(model).__name__,
            'version': '1.0'
        }
        self.model_history.append(metadata)
        
        # Ø­ÙØ¸ ÙÙŠ Ù…Ù„Ù
        metadata_path = Path('models/model_metadata.json')
        metadata_path.parent.mkdir(exist_ok=True)
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.model_history, f, indent=4, ensure_ascii=False)

class DataPreprocessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„ØªØ­Ø¶ÙŠØ±"""
    
    def __init__(self):
        self.scaler = RobustScaler()  # Ø£ÙƒØ«Ø± Ù…Ù‚Ø§ÙˆÙ…Ø© Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©
        self.imputer = SimpleImputer(strategy='median')
        self.feature_names = []
    
    def preprocess_features(self, df: pd.DataFrame, features: List[str], fit: bool = True) -> np.ndarray:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª"""
        try:
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙ‚Ø·
            X = df[features].copy()
            self.feature_names = features
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
            if fit:
                X_imputed = self.imputer.fit_transform(X)
            else:
                X_imputed = self.imputer.transform(X)
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if fit:
                X_scaled = self.scaler.fit_transform(X_imputed)
            else:
                X_scaled = self.scaler.transform(X_imputed)
            
            return X_scaled
            
        except Exception as e:
            raise Exception(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    
    def get_feature_importance(self, model) -> Dict[str, float]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª"""
        try:
            if hasattr(model, 'feature_importances_'):
                importance_dict = dict(zip(self.feature_names, model.feature_importances_))
                return dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))
            return {}
        except Exception as e:
            logging.warning(f"Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª: {e}")
            return {}

class AdvancedFailurePredictor:
    """Ù†Ù…ÙˆØ°Ø¬ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨ÙØ´Ù„ Ø§Ù„Ù…Ø¶Ø®Ø§Øª"""
    
    def __init__(self):
        self.model_manager = ModelManager()
        self.preprocessor = DataPreprocessor()
        self.model = None
        self.is_trained = False
        self.accuracy = 0.0
        self.feature_importance = {}
        self.model_type = "XGBoost"
        self.load_model()
    
    def load_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹"""
        try:
            model_path = AI_MODELS_CONFIG['failure_prediction']['model_path']
            preprocessor_path = model_path.parent / 'preprocessor.joblib'
            
            if model_path.exists() and preprocessor_path.exists():
                self.model = joblib.load(model_path)
                self.preprocessor = joblib.load(preprocessor_path)
                self.is_trained = True
                self.model_manager.logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ø¨Ù†Ø¬Ø§Ø­")
                
                # ØªØ­Ù…ÙŠÙ„ Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
                metadata_path = Path('models/model_metadata.json')
                if metadata_path.exists():
                    with open(metadata_path, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                        if metadata:
                            self.accuracy = metadata[-1].get('accuracy', 0.0)
            else:
                self._initialize_new_model()
                
        except Exception as e:
            self.model_manager.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            self._initialize_new_model()
    
    def _initialize_new_model(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯"""
        self.model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=8,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            eval_metric='logloss'
        )
        self.model_manager.logger.info("ğŸ†• ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯")
    
    def generate_training_data(self, num_samples: int = 15000) -> pd.DataFrame:
        """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ù…Ø­Ø§ÙƒØ§Ø© Ø£ÙƒØ«Ø± ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        np.random.seed(42)
        
        # Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙƒØ«Ø± ÙˆØ§Ù‚Ø¹ÙŠØ© Ù…Ø¹ Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
        operating_hours = np.random.exponential(2000, num_samples)
        
        data = {
            'vibration_x': self._generate_vibration_data(operating_hours, num_samples, base=2.0),
            'vibration_y': self._generate_vibration_data(operating_hours, num_samples, base=2.2),
            'vibration_z': self._generate_vibration_data(operating_hours, num_samples, base=1.8),
            'temperature': self._generate_temperature_data(operating_hours, num_samples),
            'pressure': self._generate_pressure_data(operating_hours, num_samples),
            'flow_rate': self._generate_flow_rate_data(operating_hours, num_samples),
            'power_consumption': self._generate_power_data(operating_hours, num_samples),
            'bearing_temperature': self._generate_bearing_temperature(operating_hours, num_samples),
            'oil_level': self._generate_oil_level_data(operating_hours, num_samples),
            'oil_quality': self._generate_oil_quality_data(operating_hours, num_samples),
            'operating_hours': operating_hours,
            'maintenance_due': np.random.choice([0, 1], num_samples, p=[0.7, 0.3])
        }
        
        df = pd.DataFrame(data)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù (Ø§Ù„ÙØ´Ù„) Ù…Ø¹ Ù…Ù†Ø·Ù‚ Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹
        df['failure'] = self._calculate_failure_risk(df)
        
        return df
    
    def _generate_vibration_data(self, operating_hours: np.ndarray, num_samples: int, base: float) -> np.ndarray:
        """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù‡ØªØ²Ø§Ø² ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        vibration = np.zeros(num_samples)
        for i, hours in enumerate(operating_hours):
            if hours > 4000:  # Ù…Ø¶Ø®Ø© Ù‚Ø¯ÙŠÙ…Ø©
                vibration[i] = np.random.normal(base * 2.5, 1.0)
            elif hours > 2000:  # Ù…Ø¶Ø®Ø© Ù…ØªÙˆØ³Ø·Ø© Ø§Ù„Ø¹Ù…Ø±
                vibration[i] = np.random.normal(base * 1.5, 0.7)
            else:  # Ù…Ø¶Ø®Ø© Ø¬Ø¯ÙŠØ¯Ø©
                vibration[i] = np.random.normal(base, 0.3)
        return np.clip(vibration, 0, 10)
    
    def _generate_temperature_data(self, operating_hours: np.ndarray, num_samples: int) -> np.ndarray:
        """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¯Ø±Ø¬Ø© Ø­Ø±Ø§Ø±Ø© ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        temperature = np.zeros(num_samples)
        for i, hours in enumerate(operating_hours):
            base_temp = 60 + (hours / 5000) * 30  # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ù…Ø¹ Ø§Ù„Ø¹Ù…Ø±
            temperature[i] = np.random.normal(base_temp, 8)
        return np.clip(temperature, 40, 120)
    
    def _generate_pressure_data(self, operating_hours: np.ndarray, num_samples: int) -> np.ndarray:
        """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¶ØºØ· ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        pressure = np.zeros(num_samples)
        for i, hours in enumerate(operating_hours):
            if hours > 3000:
                pressure[i] = np.random.normal(130, 35)  # Ø¶ØºØ· Ù…Ù†Ø®ÙØ¶ Ù„Ù„Ù…Ø¶Ø®Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            else:
                pressure[i] = np.random.normal(150, 20)
        return np.clip(pressure, 50, 250)
    
    def _generate_flow_rate_data(self, operating_hours: np.ndarray, num_samples: int) -> np.ndarray:
        """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹Ø¯Ù„ ØªØ¯ÙÙ‚ ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        flow_rate = np.zeros(num_samples)
        for i, hours in enumerate(operating_hours):
            if hours > 3500:
                flow_rate[i] = np.random.normal(70, 30)  # ØªØ¯ÙÙ‚ Ø£Ù‚Ù„ Ù„Ù„Ù…Ø¶Ø®Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            else:
                flow_rate[i] = np.random.normal(100, 15)
        return np.clip(flow_rate, 20, 150)
    
    def _generate_power_data(self, operating_hours: np.ndarray, num_samples: int) -> np.ndarray:
        """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø·Ø§Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        power = np.zeros(num_samples)
        for i, hours in enumerate(operating_hours):
            base_power = 70 + (hours / 5000) * 25  # Ø²ÙŠØ§Ø¯Ø© Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø·Ø§Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø¹Ù…Ø±
            power[i] = np.random.normal(base_power, 12)
        return np.clip(power, 50, 150)
    
    def _generate_bearing_temperature(self, operating_hours: np.ndarray, num_samples: int) -> np.ndarray:
        """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø±Ø§Ø±Ø© Ø§Ù„Ù…Ø­Ø§Ù…Ù„ ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        bearing_temp = np.zeros(num_samples)
        for i, hours in enumerate(operating_hours):
            base_temp = 65 + (hours / 5000) * 25  # Ø²ÙŠØ§Ø¯Ø© Ø­Ø±Ø§Ø±Ø© Ø§Ù„Ù…Ø­Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¹Ù…Ø±
            bearing_temp[i] = np.random.normal(base_temp, 10)
        return np.clip(bearing_temp, 50, 110)
    
    def _generate_oil_level_data(self, operating_hours: np.ndarray, num_samples: int) -> np.ndarray:
        """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø²ÙŠØª ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        oil_level = np.zeros(num_samples)
        for i, hours in enumerate(operating_hours):
            if hours > 2500:
                oil_level[i] = np.random.uniform(0.2, 0.7)  # Ù…Ø³ØªÙˆÙ‰ Ø²ÙŠØª Ù…Ù†Ø®ÙØ¶ Ù„Ù„Ù…Ø¶Ø®Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            else:
                oil_level[i] = np.random.uniform(0.6, 1.0)
        return np.clip(oil_level, 0.1, 1.0)
    
    def _generate_oil_quality_data(self, operating_hours: np.ndarray, num_samples: int) -> np.ndarray:
        """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬ÙˆØ¯Ø© Ø§Ù„Ø²ÙŠØª ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        oil_quality = np.zeros(num_samples)
        for i, hours in enumerate(operating_hours):
            base_quality = 0.9 - (hours / 10000)  # ØªØ¯Ù‡ÙˆØ± Ø¬ÙˆØ¯Ø© Ø§Ù„Ø²ÙŠØª Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª
            oil_quality[i] = np.random.uniform(max(0.1, base_quality - 0.2), base_quality)
        return np.clip(oil_quality, 0.1, 1.0)
    
    def _calculate_failure_risk(self, df: pd.DataFrame) -> np.ndarray:
        """Ø­Ø³Ø§Ø¨ Ø®Ø·Ø± Ø§Ù„ÙØ´Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø¹Ø§ÙŠÙŠØ±"""
        risk_score = (
            (df['vibration_x'] > 4.0).astype(int) * 0.15 +
            (df['vibration_y'] > 4.0).astype(int) * 0.15 +
            (df['vibration_z'] > 4.0).astype(int) * 0.15 +
            (df['temperature'] > 80).astype(int) * 0.2 +
            (df['oil_level'] < 0.3).astype(int) * 0.15 +
            (df['oil_quality'] < 0.4).astype(int) * 0.1 +
            (df['bearing_temperature'] > 85).astype(int) * 0.1
        )
        
        # ØªØ­ÙˆÙŠÙ„ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·Ø± Ø¥Ù„Ù‰ ØªÙˆÙ‚Ø¹ ÙØ´Ù„ (0 Ø£Ùˆ 1)
        failure = (risk_score > 0.3).astype(int)
        return failure
    
    def train_model(self, use_cross_validation: bool = True) -> Dict[str, Any]:
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"""
        try:
            self.model_manager.logger.info("ğŸ“ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
            
            # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            training_data = self.generate_training_data()
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            features = AI_MODELS_CONFIG['failure_prediction']['features']
            X = training_data[features]
            y = training_data['failure']
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_train_processed = self.preprocessor.preprocess_features(X_train, features, fit=True)
            X_test_processed = self.preprocessor.preprocess_features(X_test, features, fit=False)
            
            if use_cross_validation:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªØ¨Ø§Ø¯Ù„
                cv_scores = cross_val_score(self.model, X_train_processed, y_train, cv=5, scoring='accuracy')
                self.model_manager.logger.info(f"ğŸ“Š Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªØ¨Ø§Ø¯Ù„: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
            
            # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            self.model.fit(X_train_processed, y_train)
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            y_pred = self.model.predict(X_test_processed)
            y_pred_proba = self.model.predict_proba(X_test_processed)[:, 1]
            
            # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…ØªØ¹Ø¯Ø¯Ø©
            self.accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred)
            recall = recall_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred)
            
            self.model_manager.logger.info(f"âœ… Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {self.accuracy:.4f}")
            self.model_manager.logger.info(f"ğŸ“ˆ Ø§Ù„Ø¯Ù‚Ø© (Precision): {precision:.4f}")
            self.model_manager.logger.info(f"ğŸ“Š Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ (Recall): {recall:.4f}")
            self.model_manager.logger.info(f"ğŸ¯ F1-Score: {f1:.4f}")
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª
            self.feature_importance = self.preprocessor.get_feature_importance(self.model)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬
            model_path = AI_MODELS_CONFIG['failure_prediction']['model_path']
            model_path.parent.mkdir(parents=True, exist_ok=True)
            
            joblib.dump(self.model, model_path)
            joblib.dump(self.preprocessor, model_path.parent / 'preprocessor.joblib')
            
            # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
            self.model_manager.save_model_metadata(self.model, self.accuracy, features)
            
            self.is_trained = True
            self.model_manager.logger.info("ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")
            
            return {
                'accuracy': self.accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'feature_importance': self.feature_importance,
                'cv_scores': cv_scores.tolist() if use_cross_validation else []
            }
            
        except Exception as e:
            self.model_manager.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            raise
    
    def predict_failure(self, sensor_data: Dict[str, float]) -> Dict[str, Any]:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ´Ù„ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø£Ø®Ø·Ø§Ø¡"""
        if not self.is_trained:
            self.model_manager.logger.warning("âš ï¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø±Ø¨ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ...")
            self.train_model()
        
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
            missing_features = []
            input_features = []
            
            for feature in AI_MODELS_CONFIG['failure_prediction']['features']:
                value = sensor_data.get(feature)
                if value is None:
                    missing_features.append(feature)
                    input_features.append(0)  # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
                else:
                    input_features.append(float(value))
            
            if missing_features:
                self.model_manager.logger.warning(f"âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙÙ‚ÙˆØ¯Ø©: {missing_features}")
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙ†Ø¨Ø¤
            input_df = pd.DataFrame([input_features], columns=AI_MODELS_CONFIG['failure_prediction']['features'])
            input_processed = self.preprocessor.preprocess_features(input_df, AI_MODELS_CONFIG['failure_prediction']['features'], fit=False)
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤
            failure_probability = self.model.predict_proba(input_processed)[0][1]
            prediction = self.model.predict(input_processed)[0]
            
            # ØªØ­Ø³ÙŠÙ† ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø©
            risk_level, risk_color = self._calculate_risk_level(failure_probability, sensor_data)
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ÙØ´Ù„
            failure_type = self._determine_failure_type(sensor_data)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª
            recommendations = self._generate_recommendations(sensor_data, failure_probability, risk_level)
            
            # ÙˆÙ‚Øª Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„Ù…Ù‚ØªØ±Ø­
            maintenance_timing = self._suggest_maintenance_timing(failure_probability, sensor_data)
            
            return {
                'failure_probability': round(failure_probability, 4),
                'prediction': int(prediction),
                'predicted_failure_type': failure_type,
                'confidence': round(self._calculate_confidence(failure_probability, sensor_data), 4),
                'risk_level': risk_level,
                'risk_color': risk_color,
                'recommendations': recommendations,
                'maintenance_timing': maintenance_timing,
                'feature_contributions': self._get_feature_contributions(sensor_data),
                'timestamp': datetime.now(),
                'model_accuracy': self.accuracy,
                'missing_features': missing_features
            }
            
        except Exception as e:
            self.model_manager.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
            return self._get_error_response(str(e))
    
    def _calculate_risk_level(self, probability: float, sensor_data: Dict[str, float]) -> Tuple[str, str]:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø© Ù…Ø¹ Ø£Ù„ÙˆØ§Ù†"""
        # Ø¹ÙˆØ§Ù…Ù„ Ø¥Ø¶Ø§ÙÙŠØ© ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø©
        critical_factors = 0
        if sensor_data.get('temperature', 0) > 85:
            critical_factors += 1
        if sensor_data.get('oil_level', 0) < 0.2:
            critical_factors += 1
        if sensor_data.get('vibration_x', 0) > 6.0:
            critical_factors += 1
        
        # ØªØ¹Ø¯ÙŠÙ„ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø­Ø±Ø¬Ø©
        adjusted_probability = probability + (critical_factors * 0.1)
        
        if adjusted_probability >= 0.8 or critical_factors >= 2:
            return "Ø­Ø±Ø¬", "#dc3545"  # Ø£Ø­Ù…Ø±
        elif adjusted_probability >= 0.6:
            return "Ù…Ø±ØªÙØ¹", "#fd7e14"  # Ø¨Ø±ØªÙ‚Ø§Ù„ÙŠ
        elif adjusted_probability >= 0.4:
            return "Ù…ØªÙˆØ³Ø·", "#ffc107"  # Ø£ØµÙØ±
        elif adjusted_probability >= 0.2:
            return "Ù…Ù†Ø®ÙØ¶", "#20c997"  # Ø£Ø®Ø¶Ø± ÙØ§ØªØ­
        else:
            return "Ø·Ø¨ÙŠØ¹ÙŠ", "#198754"  # Ø£Ø®Ø¶Ø±
    
    def _calculate_confidence(self, probability: float, sensor_data: Dict[str, float]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤"""
        base_confidence = probability
        
        # Ø¹ÙˆØ§Ù…Ù„ ØªØ²ÙŠØ¯ Ø§Ù„Ø«Ù‚Ø©
        if all(key in sensor_data for key in ['temperature', 'vibration_x', 'oil_level']):
            base_confidence *= 1.1
        
        # Ø¹ÙˆØ§Ù…Ù„ ØªÙ‚Ù„Ù„ Ø§Ù„Ø«Ù‚Ø©
        missing_data = len([v for v in sensor_data.values() if v == 0])
        if missing_data > 3:
            base_confidence *= 0.8
        
        return min(base_confidence, 0.95)
    
    def _determine_failure_type(self, sensor_data: Dict[str, float]) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ÙØ´Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ø¨Ø¯Ù‚Ø©"""
        failure_types = []
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹ØªØ¨Ø§Øª Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª
        vibration_threshold = 4.5 + (self.feature_importance.get('vibration_x', 0) * 2)
        temperature_threshold = 80 + (self.feature_importance.get('temperature', 0) * 10)
        
        if sensor_data.get('vibration_x', 0) > vibration_threshold:
            failure_types.append("Ø¹Ø¯Ù… Ø§ØªØ²Ø§Ù† Ø§Ù„Ù…Ø­ÙˆØ± X")
        if sensor_data.get('vibration_y', 0) > vibration_threshold:
            failure_types.append("Ø¹Ø¯Ù… Ø§ØªØ²Ø§Ù† Ø§Ù„Ù…Ø­ÙˆØ± Y")
        if sensor_data.get('vibration_z', 0) > vibration_threshold:
            failure_types.append("Ø¹Ø¯Ù… Ø§ØªØ²Ø§Ù† Ø§Ù„Ù…Ø­ÙˆØ± Z")
        if sensor_data.get('temperature', 0) > temperature_threshold:
            failure_types.append("Ø§Ø±ØªÙØ§Ø¹ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©")
        if sensor_data.get('oil_level', 0) < 0.3:
            failure_types.append("Ù†Ù‚Øµ Ø§Ù„Ø²ÙŠØª")
        if sensor_data.get('oil_quality', 0) < 0.4:
            failure_types.append("ØªÙ„ÙˆØ« Ø§Ù„Ø²ÙŠØª")
        if sensor_data.get('bearing_temperature', 0) > 85:
            failure_types.append("ØªÙ„Ù Ø§Ù„Ù…Ø­Ø§Ù…Ù„")
        if sensor_data.get('flow_rate', 0) < 50:
            failure_types.append("Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„ÙƒÙØ§Ø¡Ø©")
        
        return "ØŒ ".join(failure_types) if failure_types else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø¹Ø·Ø§Ù„ ÙˆØ§Ø¶Ø­Ø©"
    
    def _generate_recommendations(self, sensor_data: Dict[str, float], probability: float, risk_level: str) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø°ÙƒÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        recommendations = []
        priority = 1
        
        # ØªÙˆØµÙŠØ§Øª Ø¹Ø§Ø¬Ù„Ø©
        if risk_level in ["Ø­Ø±Ø¬", "Ù…Ø±ØªÙØ¹"]:
            recommendations.append(f"{priority}. Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¶Ø®Ø© ÙÙˆØ±Ø§Ù‹ ÙˆØ§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ")
            priority += 1
        
        if sensor_data.get('oil_level', 0) < 0.2:
            recommendations.append(f"{priority}. Ø¥Ø¶Ø§ÙØ© Ø²ÙŠØª Ø¹Ø§Ø¬Ù„ (Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ù…Ù†Ø®ÙØ¶ Ø¬Ø¯Ø§Ù‹)")
            priority += 1
        
        if sensor_data.get('temperature', 0) > 90:
            recommendations.append(f"{priority}. ØªØ¨Ø±ÙŠØ¯ Ø¹Ø§Ø¬Ù„ Ù„Ù„Ù…Ø¶Ø®Ø©")
            priority += 1
        
        # ØªÙˆØµÙŠØ§Øª ÙˆÙ‚Ø§Ø¦ÙŠØ©
        if probability > 0.6:
            recommendations.append(f"{priority}. Ø¬Ø¯ÙˆÙ„Ø© ØµÙŠØ§Ù†Ø© Ø¹Ø§Ø¬Ù„Ø© Ø®Ù„Ø§Ù„ 24 Ø³Ø§Ø¹Ø©")
            priority += 1
        elif probability > 0.4:
            recommendations.append(f"{priority}. Ø¬Ø¯ÙˆÙ„Ø© ØµÙŠØ§Ù†Ø© Ø®Ù„Ø§Ù„ 3 Ø£ÙŠØ§Ù…")
            priority += 1
        elif probability > 0.2:
            recommendations.append(f"{priority}. ØµÙŠØ§Ù†Ø© ÙˆÙ‚Ø§Ø¦ÙŠØ© Ø®Ù„Ø§Ù„ Ø£Ø³Ø¨ÙˆØ¹")
            priority += 1
        
        if sensor_data.get('oil_quality', 0) < 0.5:
            recommendations.append(f"{priority}. Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø²ÙŠØª ÙÙŠ Ø£Ù‚Ø±Ø¨ ÙØ±ØµØ©")
            priority += 1
        
        if any(v > 4.0 for v in [sensor_data.get('vibration_x', 0), 
                                sensor_data.get('vibration_y', 0), 
                                sensor_data.get('vibration_z', 0)]):
            recommendations.append(f"{priority}. ÙØ­Øµ Ø§Ù„ØªÙˆØ§Ø²Ù† ÙˆØ§Ù„Ù…Ø­Ø§Ù…Ù„")
            priority += 1
        
        if not recommendations:
            recommendations.append("Ø§Ù„Ù…Ø¶Ø®Ø© ØªØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ - Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¯ÙˆØ±ÙŠØ©")
        
        return recommendations
    
    def _suggest_maintenance_timing(self, probability: float, sensor_data: Dict[str, float]) -> str:
        """Ø§Ù‚ØªØ±Ø§Ø­ ØªÙˆÙ‚ÙŠØª Ø§Ù„ØµÙŠØ§Ù†Ø©"""
        operating_hours = sensor_data.get('operating_hours', 0)
        
        if probability > 0.7:
            return "ÙÙˆØ±ÙŠ (Ø£Ù‚Ù„ Ù…Ù† 24 Ø³Ø§Ø¹Ø©)"
        elif probability > 0.5:
            return "Ø¹Ø§Ø¬Ù„ (1-3 Ø£ÙŠØ§Ù…)"
        elif probability > 0.3:
            return "Ù‚Ø±ÙŠØ¨ (Ø£Ø³Ø¨ÙˆØ¹)"
        elif operating_hours > 3000:
            return "ÙˆÙ‚Ø§Ø¦ÙŠ (Ø´Ù‡Ø±ÙŠ)"
        else:
            return "Ø±ÙˆØªÙŠÙ†ÙŠ (ÙƒÙ„ 3 Ø£Ø´Ù‡Ø±)"
    
    def _get_feature_contributions(self, sensor_data: Dict[str, float]) -> Dict[str, float]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ù‡Ù…Ø© ÙƒÙ„ Ù…ÙŠØ²Ø© ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤"""
        contributions = {}
        try:
            for feature, importance in self.feature_importance.items():
                value = sensor_data.get(feature, 0)
                # Ø­Ø³Ø§Ø¨ Ù…Ø³Ø§Ù‡Ù…Ø© ØªÙ‚Ø±ÙŠØ¨ÙŠØ© (ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ù‡Ø°Ø§ Ø§Ù„Ù…Ù†Ø·Ù‚)
                contributions[feature] = round(value * importance * 10, 4)
        except Exception:
            pass
        
        return contributions
    
    def _get_error_response(self, error_msg: str) -> Dict[str, Any]:
        """Ø¥Ø±Ø¬Ø§Ø¹ Ø±Ø¯ Ø®Ø·Ø£ Ù…Ù†Ø¸Ù…"""
        return {
            'failure_probability': 0.0,
            'prediction': 0,
            'predicted_failure_type': 'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤',
            'confidence': 0.0,
            'risk_level': 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ',
            'risk_color': '#6c757d',
            'recommendations': ['ÙØ­Øµ Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ', 'Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø³Ø¬Ù„Ø§Øª'],
            'maintenance_timing': 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',
            'feature_contributions': {},
            'timestamp': datetime.now(),
            'model_accuracy': 0.0,
            'error': error_msg
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        return {
            'is_trained': self.is_trained,
            'accuracy': self.accuracy,
            'model_type': self.model_type,
            'feature_importance': self.feature_importance,
            'last_trained': self.model_manager.model_history[-1]['timestamp'] if self.model_manager.model_history else 'ØºÙŠØ± Ù…ØªÙˆÙØ±',
            'features_count': len(AI_MODELS_CONFIG['failure_prediction']['features'])
        }

class AdvancedAnomalyDetector:
    """ÙƒØ§Ø´Ù Ø´Ø°ÙˆØ° Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self):
        self.model = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_estimators=100
        )
        self.scaler = RobustScaler()
        self.is_trained = False
        self.logger = logging.getLogger(__name__)
    
    def detect_anomalies(self, sensor_data: pd.DataFrame, sensitivity: float = 0.5) -> pd.DataFrame:
        """ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ù…Ø¹ Ù‚Ø§Ø¨Ù„ÙŠØ© Ø¶Ø¨Ø· Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ©"""
        try:
            if len(sensor_data) < 20:
                self.logger.warning("Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°")
                sensor_data['anomaly'] = False
                sensor_data['anomaly_score'] = 0.0
                sensor_data['anomaly_severity'] = 'low'
                return sensor_data
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            features = AI_MODELS_CONFIG['failure_prediction']['features']
            X = sensor_data[features].fillna(method='ffill').fillna(0)
            X_scaled = self.scaler.fit_transform(X)
            
            # Ø¶Ø¨Ø· Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ©
            contamination = 0.05 + (sensitivity * 0.1)  # 0.05 Ø¥Ù„Ù‰ 0.15
            self.model.set_params(contamination=min(contamination, 0.2))
            
            # Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ°
            anomalies = self.model.fit_predict(X_scaled)
            scores = self.model.decision_function(X_scaled)
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            sensor_data['anomaly'] = anomalies == -1
            sensor_data['anomaly_score'] = scores
            sensor_data['anomaly_severity'] = sensor_data['anomaly_score'].apply(
                lambda x: 'high' if x < -0.1 else 'medium' if x < 0 else 'low'
            )
            
            self.is_trained = True
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            anomaly_count = sensor_data['anomaly'].sum()
            self.logger.info(f"ØªÙ… ÙƒØ´Ù {anomaly_count} Ø­Ø§Ù„Ø© Ø´Ø°ÙˆØ° Ù…Ù† {len(sensor_data)} Ø³Ø¬Ù„")
            
            return sensor_data
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°: {e}")
            return sensor_data

# Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø® Ø¹Ø§Ù…Ø© Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø·ÙˆØ±Ø©
failure_predictor = AdvancedFailurePredictor()
anomaly_detector = AdvancedAnomalyDetector()